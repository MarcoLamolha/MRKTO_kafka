{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales ETL and Real-Time KPI Dashboard\n",
    "\n",
    "This project demonstrates an end-to-end ETL pipeline, from data generation to a real-time KPI dashboard, showcasing data engineering and processing skills. \n",
    "\n",
    "The project leverages a combination of cloud storage, ETL practices, and real-time data visualization to simulate a sales pipeline. \n",
    "\n",
    "This README details the projectâ€™s architecture, main components, and instructions for running the project locally or in Docker containers.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Project Overview](#project-overview)\n",
    "- [Technologies and Libraries Used](#technologies-and-libraries-used)\n",
    "- [Architecture](#architecture)\n",
    "- [Setup Instructions](#setup-instructions)\n",
    "- [Data Flow](#data-flow)\n",
    "- [Key Performance Indicators (KPIs)](#key-performance-indicators-kpis)\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "The project simulates a daily sales sheet generation system where a CSV file is created to represent daily sales data. This data is uploaded to an AWS S3 bucket, which acts as the staging area. A scheduled process monitors this bucket for new files. Once detected, an ETL pipeline is initiated to:\n",
    "1. **Extract**: Retrieve the file from the S3 bucket.\n",
    "2. **Transform**: Clean and validate data using `pandas` and `pydantic` for data integrity.\n",
    "3. **Load**: Insert the data into a PostgreSQL database.\n",
    "\n",
    "The data is then visualized in a Streamlit-powered web application that accesses the PostgreSQL database and displays key sales KPIs in real time.\n",
    "\n",
    "![architecture](![image](![app](https://github.com/user-attachments/assets/066cd9d0-4056-4812-9109-7f7ad9c09028)\n",
    "\n",
    "## Technologies and Libraries Used\n",
    "\n",
    "### Infrastructure\n",
    "- **AWS S3**: Storage of sales data files, acting as the data source for ETL.\n",
    "- **PostgreSQL**: Storage for transformed data, enabling real-time data retrieval.\n",
    "- **Docker**: Containerization of services for easy deployment and management.\n",
    "- **Kafka**: Message queuing system for monitoring and triggering ETL processes.\n",
    "\n",
    "### Development and Analysis Tools\n",
    "- **DBeaver**: Database management and querying.\n",
    "  \n",
    "### Main Python Libraries\n",
    "- **pandas**: Data manipulation and transformation.\n",
    "- **boto3**: AWS SDK for Python, used to interact with S3.\n",
    "- **Faker**: Simulation of sales data.\n",
    "- **pydantic**: Data validation, ensuring data quality in each pipeline step.\n",
    "- **sqlalchemy**: ORM for data insertion into PostgreSQL.\n",
    "- **streamlit**: Real-time KPI dashboard.\n",
    "- **confluent_kafka**: Interface for Kafka, handling event-driven ETL execution.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "This project follows a modular ETL pipeline and visualization architecture. Each component is responsible for a specific function:\n",
    "\n",
    "![architecture](![image](https://github.com/user-attachments/assets/2e2d9a63-51da-42b4-b245-65fdf8cd7941)\n",
    "\n",
    "\n",
    "1. **Data Generation**: `csv_generator.py` creates a sales data CSV with simulated daily sales.\n",
    "2. **S3 Storage and Monitoring**: Files are stored in an S3 bucket and monitored by a Kafka topic that triggers the ETL process upon file arrival.\n",
    "3. **ETL Pipeline**:\n",
    "   - **Extraction**: Pulls CSV from S3.\n",
    "   - **Transformation**: Validates and cleans data using `pydantic`.\n",
    "   - **Loading**: Inserts data into PostgreSQL.\n",
    "4. **Visualization**: A Streamlit application that pulls from PostgreSQL to provide real-time KPIs.\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Docker\n",
    "- AWS CLI configured with S3 access\n",
    "- PostgreSQL\n",
    "- Kafka\n",
    "  \n",
    "### Running the Project\n",
    "\n",
    "1. **Clone the repository**:\n",
    "   ```bash\n",
    "   git clone git@github.com/caio-moliveira/sales-pipeline-project.git\n",
    "   cd sales-pipeline-project\n",
    "   ```\n",
    "2. **Create and configure** `.env` **file** for PostgreSQL, AWS, and Kafka settings.\n",
    "\n",
    "\n",
    "3. **Run Docker Services**\n",
    "  \n",
    "  ```bash\n",
    "  docker-compose up --build\n",
    "  ```\n",
    "\n",
    "4. **Environment Variables**\n",
    "\n",
    "This project requires specific environment variables to be set for proper functioning. Below is a description of each variable and its purpose. Make sure to include them in a `.env` file at the root of the project.\n",
    "\n",
    "### AWS Configuration\n",
    "- **`AWS_ACCESS_KEY_ID`**: Your AWS access key for authentication.\n",
    "- **`AWS_SECRET_ACCESS_KEY`**: Your AWS secret access key.\n",
    "- **`AWS_REGION`**: The AWS region where your services are hosted (e.g., `us-east-1`).\n",
    "- **`BUCKET_NAME`**: The name of your AWS S3 bucket.\n",
    "\n",
    "### PostgreSQL Database Configuration\n",
    "- **`POSTGRES_USER`**: The username for your PostgreSQL database.\n",
    "- **`POSTGRES_PASSWORD`**: The password for your PostgreSQL database.\n",
    "- **`POSTGRES_HOST`**: The host address of your PostgreSQL database.\n",
    "- **`POSTGRES_DB`**: The name of your PostgreSQL database.\n",
    "\n",
    "### Kafka Configuration\n",
    "- **`BOOTSTRAP_SERVERS`**: The Kafka broker(s) to connect to (e.g., `broker1:9092,broker2:9092`).\n",
    "- **`SASL_USERNAME`**: Your Kafka username.\n",
    "- **`SASL_PASSWORD`**: Your Kafka password.\n",
    "- **`CLIENT_ID`**: The unique identifier for the Kafka client.\n",
    "\n",
    "#### Setting Up\n",
    "1. Create a `.env` file in the root of the project.\n",
    "2. Copy the example below and replace placeholder values with your actual credentials.\n",
    "3. Ensure the `.env` file is excluded from version control using `.gitignore` to keep sensitive information private.\n",
    "\n",
    "### Example `.env` File\n",
    "```env\n",
    "AWS_ACCESS_KEY_ID=your_aws_access_key\n",
    "AWS_SECRET_ACCESS_KEY=your_aws_secret_key\n",
    "AWS_REGION=your_aws_region\n",
    "BUCKET_NAME=your_bucket_name\n",
    "\n",
    "POSTGRES_USER=your_postgres_username\n",
    "POSTGRES_PASSWORD=your_postgres_password\n",
    "POSTGRES_HOST=your_postgres_host\n",
    "POSTGRES_DB=your_postgres_database\n",
    "\n",
    "BOOTSTRAP_SERVERS=your_kafka_bootstrap_servers\n",
    "SASL_USERNAME=your_kafka_username\n",
    "SASL_PASSWORD=your_kafka_password\n",
    "CLIENT_ID=your_kafka_client_id\n",
    "```\n",
    "\n",
    " ## Data Flow\n",
    "\n",
    "1. **Data Generation**: \n",
    "   - Uses `Faker` to simulate realistic sales data, stored as a CSV in S3.\n",
    "2. **Extraction**:\n",
    "   - `boto3` fetches the latest sales CSV from S3.\n",
    "3. **Transformation**:\n",
    "   - `pandas` and `pydantic` perform basic data cleaning and validation.\n",
    "4. **Loading**:\n",
    "   - `sqlalchemy` ORM maps data to PostgreSQL.\n",
    "5. **Visualization**:\n",
    "   - Streamlit dashboard connects to PostgreSQL, presenting KPIs in real time.\n",
    "\n",
    "## Key Performance Indicators (KPIs)\n",
    "\n",
    "The dashboard displays the following KPIs:\n",
    "- **Total Sales**: Sum of daily sales.\n",
    "- **Average Transaction Value**: Average value of transactions.\n",
    "- **Top-selling Products**: Highest volume products for the day.\n",
    "- **Sales by Category**: Breakdown of sales by product category.\n",
    "- **Sales Trend**: A real-time chart tracking sales over time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
